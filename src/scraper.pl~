% Copyright Robert Ryley 2016.  Released under LGPL License (v3).
% Revised 8/17/2016
%
% Cleanup and generalization of html parser program.
% Program description: Accept as input a file or URL, and extract
% specific components based on file extension (ie. PDF, jpeg, MP3, txt,
% etc), and save locally. User will have option to specify path and file
% name for each item, or accept default local file name, derived from
% component extension.
%
% Requirements: must be able to parse arbitrary HTML/XML and search for
% actual text element, or attribute, allow user to modify listing of
% data scraped and save results locally.
%
%   To start reading source, go to bottom of file to start procedure and
%   read in reverse order to this section.
%   To run program, at toplevel	enter '['path/to/scraper2.pl'].'
%   and	enter "start." at prompt.

:- use_module(library(debug)).
:- use_module(library(error)).
:- use_module(library(http/http_open)).
:- use_module(library(xpath)).
:- use_module(library(uri)).

% Unused for now, but useful for testing with query_tag/3 to extract
% different types of data manually at toplevel.
:- dynamic
	doc/1,
	data/1,
        text/1,
        relative/1,
	absolute/1,
	tags/1.

% Collect Tags maps visible text for link to URL attribute metadata. It
% presumes textual data and link data have been extracted with
% query_tag/3.  To test, type at toplevel:
% ? text(X), links(Y), collect_tags(X,Y,Z).

% Test data -- remove % to experiment. New lines stripped
% for testing when writing to file.

% text(['Google Search engine,','Amazon Shopping','Swi Prolog
% Website.','Clozure Common Lisp','SBCL Common Lisp','Webbots, Spiders,
% Screenscraper Website','Barebones PHP webscraper Docs']).

% links(['https://www.google.com',
% 'https://www.amazon.com',
% 'https://www.swi-prolog.org',
% 'https://www.clozure.com','https://wwww.cons.org',
% 'http://www.schrenk.com/nostarch/webbots/DSP_targets.php',
% 'https://barebonescms.com/documentation/ultimate_web_scraper_toolkit/']).


/*   Query tag -- interface to Xpath

query_tag(help) :-  document arguments query tag takes.

query_tag(doc(DOM), <tag>, Results) :- find all tags in document that
unify with <tag>.

*/


query_tag(DOM, Tag, Results) :- doc(DOM),
	findall(Query, xpath(DOM, Tag, Query), Results).


% AO - cleaned up by using handy property of strings - they handle
% newlines

query_tag(help) :- write("The predicate query_tag/3 returns portions of a previously parsed HTML/XML
document specified by the first argument.  The proper format for a query tag is
//tag -- returns all items matching that tag sequentially.
//tag(Int) -- returns specific tag or tags matching query.
//tag(@attribute) -- returns attributes of all tags.
Examples:
?- query_tag(DOM, //a(@href), Results). -> sequentially returns links.
?- query_tag(DOM, //li(normalize_space), Results). -> text collected into list.
?- query_tag(DOM, //li(4, text), Results). -> returns the text in the fourth element in a
list.\n").



/* collect_tags([H1|Rest1],[H2|Rest2], Tags) :-  %assumes lists are of equal length. Not defined in other states.
	atomic_list_concat([H1,H2], " -- ",Out),
	append([Out], Tags),
	print(Out), nl,
	collect_tags(Rest1,Rest2, Tags).

Above prints out list in format I want, but I am not
able to pass data around correctly yet. Tags var remains uninstantiated
at end of recursion.

It appears I have to add an additional clause and variable and use a
tail recursive procedure, and unify the internal variable with my Tags
variable at the end.

Solution to try -- have collect_tags/3 call collect_tags/4 as in
collect_tags(List1,List2,[],Tags) and
collect_tags(List1,List2,Stack,Tags).

After first call, Arg 3 will have results pushed onto it as below and
will be a proper list. At termination, unify arg 3 with 4 to pass
results down stack.

Inspired by Prolog Programming in Depth, page 79 on list reversal.
*/

% Predicate collect_tags/3 assumes lists are of equal length. Not
% defined in other states.
%

% AO - corrected to standard indenting in several places
collect_tags(List_1, List_2, Tags) :-
	collect_tags_aux(List_1,List_2,[],Tags).

% Induction case:
collect_tags_aux([H1|Rest1],[H2|Rest2], Stack, Tags) :-  %Stack instantiated to [] (empty list) on first call
	% AO - more standard (and fixed comment)
	atomic_list_concat([H1,' -- ', H2],New),	 % Takes element from each lists and makes new atom separated by " -- ".
	collect_tags_aux(Rest1,Rest2, [New|Stack], Tags). %Stack instantiated to empty list, with [New] as head.
							  %Later calls will push result to head of list.
							  %Tags remains uninstantiated until last call.

% Base case: When both lists are empty, [New|Stack] is unified with
% Tags. Tags has merged input lists and reversed their order.

collect_tags_aux([], [], Tags, Tags) :-
	reverse(Tags, Original),                   % Input lists will be reversed again to original order
        assert(tags(Original)),                    % Data stored in DB in original order
	% AO - storing constant strings as atoms is more efficient, as atoms are interned
        write('Data stored in DB. Query tags(Var) to retrive.\n'),
	write_results(Original), nl.		   % All elements are printed to screen and procedure terminates.

% collects a subset or the complete table if all particular cells are
% specified.

collect_cells(DOM, Xpath, Start,End,N, Results) :-
	between(Start, End, N),
	query_tag(DOM, Xpath, Results).



% Filter predicates to separate relative (inbound) links from
% absolute (outbound) ones. Uses uri_components predicate from
% library(uri).	There are 4 possible cases to cover:

test_url(URL_List, Absolute, Relative) :-
	test_url_aux(URL_List, [], [], Absolute, Relative), !.

% Case 1: Absolute URL -- Scheme will be instantiated to http or https.
% This gets pushed to absolute url list.
test_url_aux([H|Rest], Abs_stack, Rel_stack, Absolute, Relative) :-
	% AO - destructure when you can and it increases clarity
	uri_components(H, uri_components(Scheme, _Authority, Path, _Search, _Fragment)),
	nonvar(Scheme),
	nonvar(Path),
	http_or_https(Scheme),
	test_url_aux(Rest, [H|Abs_stack], Rel_stack, Absolute, Relative).


% Case 2. Relative URL -- Scheme will be uninstantiated. Path will be
% instantiated.  This gets pushed to relative url list.
test_url_aux([H|Rest], Abs_stack, Rel_stack, Absolute, Relative) :-
	uri_components(H, uri_components(Scheme, _Authority, _Path, _Search, _Fragment)),
	var(Scheme),
	test_url_aux(Rest, Abs_stack, [H|Rel_stack], Absolute, Relative).

% Case 3: Scheme instantiated, but not to http or https. Simply ignore
% and call again with tail of list. May have to pass both relative and
% absolute lists throughout recursion.

test_url_aux([H|Rest], Abs_stack, Rel_stack, Absolute, Relative) :-
	uri_components(H, uri_components(Scheme, _Authority, _Path, _Search, _Fragment)),
	nonvar(Scheme),
	\+ http_or_https(Scheme),
        test_url_aux(Rest, Abs_stack, Rel_stack, Absolute, Relative).

% Case 4: Base Case -- URL List is empty.  Store results in DB.
test_url_aux([],Absolute, Relative, Absolute, Relative) :-
	reverse(Absolute, Abs_original),
	reverse(Relative, Rel_original),
	assert(absolute(Abs_original)),
	assert(relative(Rel_original)),
	write('Query absolute(X) or relative(Y) to retrive result list.\n').

http_or_https(http).
http_or_https(https).

%% To test, run at toplevel:
%% get_local_file("test_input/cliki_index.htm").
%% query_tag(DOM, //div(1)/div(1)/div(2)/ul(3)/li/a(@href), Results),
%% test_url(Results, Absolute, Relative).

% 4/8/2016: Creates absolute URLs after extraction relative URL from
% page to be tested. Needs tail recursive procedure similar to
% collect_tags.
% 8/16/2016: For spider portion, need to test for relative vs absolute
% URL. I have not concluded whether using the regular expression
% library or using a custom DCG is best. I suspect the latter.

make_url(Domain, List, Links) :-
	make_url_aux(Domain, List, [], Links).

% Induction case: H is instantiated to first element of List, Stack is
% instantiated to empty list on first call. On subsequent calls, it will
% be a proper list. List is reduced by 1 item on each call.

make_url_aux(Domain, [H|List],Stack, Links) :-
	atomic_concat(Domain, H, Out),
	make_url_aux(Domain, List, [Out|Stack], Links).

% Base Case: Stack passed to Links via unification, with data in reverse
% order. Links variable is now empty. Reverse returns Links to original
% order. _Domain is not used and is anonymous variable.

make_url_aux(_Domain, [], Links, Links) :- reverse(Links, Original),
	write_results(Original), !.

/*  Write utility procedures for lists */

% write to standard output
% write_results predicated added with variable instantiation test.
% if an uninstantiated variable is passed, this will be trapped and
% an error message displayed. Passing uninstantiated variables to the
% recursive predicate results in an infinite loop.

write_results(H) :-
	var(H),
	write('Data source not instantiated.\n').

write_results([H|Rest]) :-
	write(H),
	nl,
        write_results(Rest).

write_results([]) :-
	write('Done'),
	nl,
	!.



% write to file stream. Saves only the most recently asserted results.

write_results_to_file(_Stream, H) :-
	var(H),
	write('Data source not instantiated.\n').

write_results_to_file(Stream, [H|Rest]) :-
	write(Stream, H),
	write(Stream, '\n'),
        write_results_to_file(Stream, Rest).

write_results_to_file(Stream, []) :-
	write(Stream, '\n'), !.



% Table data extraction predicates
dump_table(File, Xpath, First, Last, N) :-
    doc(DOM),
    open(File, write, Handle),
    between(First,Last,N),
    xpath(DOM, Xpath, Data), % N will be user supplied in Xpath uninstantiated
	write_table(Handle, N, Last, Data),
	close(Data).

%Adapted from dmiles prolog irc.

write_table(Stream, Cells, Data) :-
    must_be(integer,Cells),
    must_be(list,Data),
    write_table0(Stream, Cells, Data).

write_table0(Stream, Cells, [H|Rest]) :-
    write(Stream, H),
    Remain is Cells - 1,
    write_table(Stream, Cells, Remain, Rest).

write_table(_Stream, _,N,[]):- N==0 ->   ! ; throw('table under-run').

write_table(Stream, Cells, Remain, [H|Rest]) :-
   Remain < 1
   ->
   (nl,
    write_table0(Stream, Cells, [H|Rest]))
   ;
   (write(', '),
    write(H),
    RemainM1 is Remain - 1,
	write_table(Stream, Cells, RemainM1, Rest)).



% File handling
get_local_file(File) :-
	open(File, read, Stream),
	load_html(Stream, DOM, [syntax_errors(quiet),
			        max_errors(-1)]),
	% AO - use asserta or assertz, assert is vague and deprecated
	assert(doc(DOM)),   %might not need this.
	print(File), print(' stored in database. Query doc(Var) to retrieve.'),
	close(Stream).

% AO -  make sure you get the http open before you load
get_url(URL, DOM) :-
	setup_call_cleanup(
	    http_open(URL, In, []),
            load_html(In, DOM, [syntax_errors(quiet),
			        max_errors(-1)]),
            close(In)).

% Save_to_file is correct now.  See notes below.
save_to_file(File_name, Term) :- %Term should be an instantiated list
	open(File_name, write, Data, [close_on_abort(true), create([all])]),
	write_results_to_file(Data, Term),
	close(Data).

% main procedures on start of program
start :-
	write("Use this program to extract data and metadata from webpages. For locally saved pages, use
get_local_file('/path/to/file').
to load document. For remote page, use
get_url('http://path.to.doc.bla/more/paths/if/needed', DOM).
File names and paths are single quoted strings. Arguments to predicates are
separated by comma (,) and are terminated by period (.).
To extract data, call 'query_tag(help).' for more info
To save extracted data, do the following:
?- data_1(List1), save_to_file('path/to/file', List1) \n").


% Scraper update --
% 1. need to convert regular Xpath to prolog term format
% 2. Take input file with list of URLs and Xpaths and download data to
%    CSV format.
% 3. Develop command line interface to create input file for later batch
%    processing.
% 4. Error handling -- ie. log when download fails.
%
% input file format:
% site(Name,                   %Single Quoted Atom
%      URL,                    %Single Quoted Atom
%      Output_File,            %Single Quoted Atom
%      xpath(Start, End), %Start/End are Terms. End may be atom 'no'.
%      multiple_pages(yes/no)).
%
% Scraper input files may have prolog style comments for clarification.
%
%This works at toplevel now (once Scraper is loaded).
%get_local_file('c:/Users/Greyhat/Downloads/Is This Coin A Scam.html').
% doc(X), query_tag(X, //(div)/table/tr/td(text), Results),
% save_to_file(Filename, Results).
% query_tag(DOM, //(div)/table/tbody/tr/td(text), Results),
%	csv(Rows, Results, [ignore_quotes(true), functor(coin),
%	arity(6)]).
% Error: Type error: `character' expected, found `'BTC'' (an atom)
% Solution: convert Results (list of atoms) to char list, then pass to
% csv.
























